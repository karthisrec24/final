# ğŸ“– PDF Q&A Chatbot using Streamlit & Hugging Face API

## ğŸš€ Overview
This project is a **PDF-based Q&A chatbot** built with **Streamlit**, **FAISS** for similarity search, and **Hugging Face Inference API** for generating answers. Users can upload a **PDF file**, ask questions about its content, and receive answers using an **LLM (e.g., Mistral-7B, Falcon-7B, Llama-3, etc.)**.

---

## ğŸ¯ Features
âœ… **Upload PDFs** & extract text + tables ğŸ“„  
âœ… **Vector search with FAISS** for retrieving relevant text ğŸ”  
âœ… **Generate answers using Hugging Face Inference API** ğŸ¤–  
âœ… **Streamlit UI for easy interaction** ğŸ–¥ï¸  
âœ… **Supports any Hugging Face text generation model** ğŸ”„  

---

## ğŸ› ï¸ Installation

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/ShubhamMandowara/llm_rag.git
cd pdf-qna-chatbot
```

### **2ï¸âƒ£ Create a Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### **3ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ Set Up Hugging Face API Token
1. Get your API token from [Hugging Face Tokens](https://huggingface.co/settings/tokens).
2. When running the Streamlit app, enter the API token in the provided input field.
3. Or, store it in **Streamlit secrets** by adding to `.streamlit/secrets.toml`:
   ```toml
   [secrets]
   HUGGINGFACEHUB_API_TOKEN = "your_token_here"
   ```

---

## ğŸ¯ Usage

### **Run the Streamlit App**
```bash
streamlit run app.py
```

### **How It Works**
1ï¸âƒ£ **Upload a PDF** to extract text & tables.  
2ï¸âƒ£ **FAISS retrieves relevant text chunks** from the document.  
3ï¸âƒ£ **Hugging Face Inference API** generates an answer based on context.  
4ï¸âƒ£ **Streamlit UI displays the answer.** ğŸ‰  

---

## ğŸ—ï¸ Project Structure
```
ğŸ“‚ pdf-qna-chatbot
â”‚â”€â”€ app.py              # Main Streamlit app
â”‚â”€â”€ requirements.txt    # Required dependencies
â”‚â”€â”€ secrets.toml         # Streamlit config folder (for secrets)
```

---

## ğŸ“Œ Example Models (Change in `app.py`)
- **Mistral-7B**: `mistralai/Mistral-7B-Instruct`
- **Llama-3**: `meta-llama/Llama-3-8B`
- **Falcon-7B**: `tiiuae/falcon-7b-instruct`

To change the model, update:
```python
HF_MODEL = "mistralai/Mistral-7B-Instruct"  # Change to your preferred model
```

---

## ğŸ› ï¸ Troubleshooting
### **1ï¸âƒ£ Model Not Found / API Issues**
- Ensure your **Hugging Face API token** is correct.
- Check if the model is **public** and supports the **Inference API**.
- Try testing API manually:
  ```bash
  curl -H "Authorization: Bearer your_api_token" https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct
  ```

### **2ï¸âƒ£ FAISS Index Not Working**
- Ensure FAISS is installed:
  ```bash
  pip install faiss-cpu  # Or faiss-gpu if using CUDA
  ```

### **3ï¸âƒ£ Streamlit Not Running**
- Restart the app after setting environment variables:
  ```bash
  streamlit run app.py
  ```

---

## ğŸ“œ License
This project is **open-source** under the GPL-3.0 License.

---

## ğŸ‘¨â€ğŸ’» Author
Developed by [Shubham Mandowara](https://github.com/shubhammandowara). Contributions are welcome! ğŸ˜ŠğŸš€